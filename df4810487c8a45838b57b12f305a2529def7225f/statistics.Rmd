---
title: "Statistics"
---

```{r eval = TRUE,  message=F, include=F, warning=F, purl=F, results="hide"}
knitr::purl('statistics.Rmd', documentation = F)
```

```{r echo=FALSE}
xaringanExtra::use_clipboard()
```

```{r echo=FALSE, purl=F}
xfun::embed_file('statistics.Rmd')
```

```{r echo=FALSE, purl=F}
xfun::embed_file('statistics.R')
```


```{r,  eval=T, warning=F, message=F}
library (psych)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(car)
```

<center>
![](Figures/Sadistics.png){width=50%}
</center>

# Descriptive

## all-in-one

Let's go back to the simple `students` data set, but you can easily imagine now that it can be replaced by a much more complex [data set](./dataset.html). 


```{r,  eval=T}
# students data set url 
students<-read.table('https://www.dipintothereef.com/uploads/3/7/3/5/37359245/students.txt',header=T, sep="\t", dec='.') 
# to write it:
write.table(students, file = "Data/students.txt", sep = "\t", row.names=T)
```

Descriptive statistics are extracting basic information from a data set. They are usually used to get a quick idea of possible differences, data distribution, or just summarize them. They are easy to extract on a single variable, but it may be a bit more complicated to get the same information on multiple variables at once. If you are familiar with the pivot table functionality in excel, the same can easily accomplished in R such as here using the functions  as `describe` or `describeBy`. Let's explore our `students` data set


+ A quick `summary` of our data set in R (review): 

```{r,  eval=T}
summary(students)
```

+ A more detailed summary can be obtained using the function `describe` from the package `psych`.

```{r,  eval=T}
psych::describe(students)
```

+ The function `describeBy` further provides a group-wise summary of the dataset.

```{r,  eval=T}
psych::describeBy (students,students$gender)
psych::describeBy (students, list(students$gender,students$population))
```

## Counts & proportions

Simple tabulation (count) summary are made using`table`

```{r,  eval=T}
# One variables
c
prop.table (table(students$gender))

# two variables
table(students$gender, students$shoesize)
prop.table (table(students$gender, students$shoesize))

# three variables, with nicer formatting
ftable(students$gender, students$shoesize,students$population)
```

## `mean`, `median`, `sd`, etc.

We previously applied a diversity of function on a selected vector, created a filter and applied it on subgroups: 

```{r,  eval=T}
mean(students$height)
ind.male <- students$gender == 'male'
mean(students$height[ind.male])
```

Descriptive statistics can be calculated on subgroups using `aggregate` and `apply` functions. The difference between the two is that the 2nd argument in `aggregate` must be a list while in `tapply` it can be a list but this is not mandatory. As a consequence, the output of `aggregate` is a data frame while the one of `tapply` is an array (basically, you deal with the list before or after).   

```{r,  eval=T}
aggregate(students$height,list (students$gender),median)
tapply(students$height,students$gender, median)
```

The wide family of `apply` functions  represents common alternatives to **loops**. You can read more about the wide application of these functions in many online resources.

<p class="comment">
**Practice 7.1** Using the data 'iris' explore how the four traits vary among flower species. (1) Use `boxplot` as visual summaries then extract descriptive statistics by species (`describeBy` or others) (2) Count number of observations in each trait and species (be creative). it should results in a table of 3 species x 4 traits filled with "50" values. (3) Calculate the `median` of each variable by `Species`, then calculate the `mean` by `Species` but for the `Sepal.Length` only
</p>

```{r, code_folding = 'Show Solution',  eval=F}
plot1 <-ggplot(iris, aes(x=Species, y=Sepal.Length)) + 
  geom_boxplot()
  
plot2 <-ggplot(iris, aes(x=Species, y=Sepal.Width)) + 
  geom_boxplot() 
  
plot3 <-ggplot(iris, aes(x=Species, y=Petal.Length)) + 
  geom_boxplot() 
  
plot4 <-ggplot(iris, aes(x=Species, y=Petal.Width)) + 
  geom_boxplot() 
  
grid.arrange(plot1, plot2,plot3, plot4, ncol=2)
describeBy (iris, iris$Species)

iris %>% group_by(Species) %>% summarise(across(c(1:4), length))
aggregate(iris[,1:4],by=list(iris$Species), median)
tapply(iris$Sepal.Length , iris$Species, mean)
```

# Hypothesis


One of the most basic concepts in statistics is hypothesis testing and something called The Null Hypothesis. Let's see more about this with Josh Starmer (a musical artist former professor at University of North Carolina at Chapel Hill) and his amazing YouTube Channel [StatQuest](https://www.youtube.com/c/joshstarmer)

You have hours of [enjoyment](https://youtube.com/playlist?list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9) to enjoy music and stats, but for the basics today this  [video ](https://www.youtube.com/watch?v=0oc49DyA3hU&list=PLblh5JKOoLUK0FLuzwntyYI10UQFUhsY9&index=8) is a perfect introduction to the topic.  

Two broad categories: parametric (assuming normal distribution & homoscedasticity - video 3) and non parametric (no assumption of normality).The most important is always your **hypotheses**: *H0* represents the null hypothesis - an absence of difference (does not mean there is no difference), *H1* represents the alternative hypothesis - the presence of a difference.

In those tests it is important to understand the concept of degrees of freedom: it refers to **the maximum number of logically independent values**, which are values that have the freedom to vary, in the data sample. Have a look [here](https://www.youtube.com/watch?v=N20rl2llHno&ab_channel=zedstatistics) if this notion is not clear to you as it is the basis you will need for later.

## Correlations

Let's first examine what is a correlation. It may look scary for some of you but it is actually very easy to compute (see a demo in step-by step demo in excel [here](.\Data\demo_stats.xlsx)).


* _Pearson_ correlation - parametric
  
  + $r$ pearson coefficient
$$
r = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}
$$
  + Formula for the $t$ test fro the correlation coefficient:
$$
t = r\sqrt\frac{n-2}{1-r^2}
$$
___________with degrees of freedom equal to $n-2$
  + Example:

```{r class.source = "fold-show",  eval=T}
# dataset hypotheses?
x<-students$height
y<-students$shoesize
s<-students[,1:2] # a matrix
# Pearson correlation
# cor(x,y)
# cor(s)
cor.test(x,y)
```

This relationship is usually represented with it confident interval on a scatter plot.

```{r,  eval=T}
ggplot(students, aes(x = height, y = shoesize)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```


* _Spearman_ correlation - non-parametric

It is based on ranking, and tests the existence of a monotonic relationship.

```{r,  eval=T}
# Spearman correlation (monotonic)
# cor(x,y, method ='spearman')
cor.test(x,y, method ='spearman')
```

**The example below illustrates on the importance of running both types of correlation** 

```{r,  eval=F}
w<-(1:100)
z<-exp(x)
cor.test(w,z,method='pearson') # super low
cor.test(w,z,method='spearman') #super high
```

<p class="comment">
**Practice 7.2** Play with modifying a value from the student data set and check how $t$ and $p$ values are affected by this change. Examine change in the confidence interval.
</p>

## Chi-square test

It is a goodness-of-fit test: observed vs theoretical. 

+ Casino example

<center>
![](Figures/die.png){width=50%}
</center>

```{r,  eval=T}
#Cast 240 times a die. We counted occurrence of 1,2,3,4,5,6
die<-data.frame(obs=c(55,44,35,45,31,30), row.names=c('Cast1','Cast2','Cast3','Cast4','Cast5','Cast6'))
die #Is this die fair? Define H0 and H1.  
```

Assumption: all results are equally probably (H0, uniform distribution).

```{r,  eval=T}
chisq.test(die)
# I am cheating
```

+ Hardy-Weinberg

A common biological application of a Chi-square test is the Hardy-Weinberg equilibrium. Genotypes at the Hardy-Weinberg equilibrium will follow: *p*2 + 2*pq* + *q*2 = 1 at panmixia (random mating, *p* and *q* represent allele frequency). In this case, we want to compare our observation to a theoretical distribution.

Let's assume 2 alleles (*A* and *T*) with observed genotypes in a population: 750 *AA*, *50* AT, 200 *TT* => *f(A)*: 0.775 / *f(T)*= 0.225. Theoretical distribution should follow: 0.60(*p2*), 0.5 (*2pq*), 0.05 (*q2*) with *p2* + *2pq* + *q2* = 1. Is our population at the HW-equilibrium (H0)?

```{r,  eval=T}
obs <- c(750, 50, 200)
exp <- c(0.60, 0.35, 0.05)
chisq.test (x=obs, p=exp)
```

+ Frequencies

Chi-square test can simply be used to compare the distribution of frequencies in two populations: 

```{r,  eval=T}
F <- matrix(nrow=4,ncol=2,data=c(33,14, 8,18,31,25,14,12))
chisq.test(F) # alternative see `fisher.test`
```

## Student t-test

Several version of t-test exist: one sample compared to a known mean, two samples (with variance equal - Student's or with unequal variance - Welsh's), **paired** (compare two dependent (e.g. before-after) samples. See also the step-by step demo in excel [here](.\Data\demo_stats.xlsx))

```{r,  eval=T}
# One sample
t.test (students$height, mu=170)
# Two sample (with equal variances)
t.test (students$height~students$gender, var.equal = TRUE)
# Two sample (with unequal variances, default option when using t.test) 
t.test (students$height~students$gender)
# Two sample paired t.test
t.test (students$height~students$gender, paired=T)
```

In the case of a two samples assuming equal variances, the *t* statistic is obtained as following:

![](Figures/ttest.png)


<p class="comment">
**Practice 7.3** Using `iris`. Does `Sepal.Lenght` differ between species `setosa` and `versicolor`? Does `Sepal.Lenght` differ between `virginica`  and `versicolor`? Make a plot, define your hypotheses, test hypotheses
</p>


```{r, code_folding = 'Show Solution',  eval=F}
set <- iris[iris$Species == "setosa", ]$Sepal.Length
ver <- iris[iris$Species == "versicolor", ]$Sepal.Length
vir <- iris[iris$Species == "virginica", ]$Sepal.Length

setver <- t.test(set, ver, paired = FALSE, alternative = "two.sided", var.equal = FALSE)
vervir <- t.test(ver, vir, paired = FALSE, alternative = "two.sided", var.equal = FALSE)
```

## Mann-Whitney

A non-parametric solution for the comparison of two samples: Mann-Whitney U-test (independent) or Wilcoxon W-test (dependant)

```{r class.source = "fold-show",  eval=T}
# Normality plot & test
students$height[6]<-132
students$height[10]<-310
students$height[8]<-132
students$height[9]<-210
boxplot(height~gender, students)
qqnorm(students$height) 
qqline(students$height) 
shapiro.test(students$height) # data are not normal 
wilcox.test (students$height~students$gender) 
```

## Variances

Variance tests are used to determine if the variances of two (or more) populations are equal. F-test are generally very sensitive to the non-normality of the data as you can imagine from the previous `boxplot`. The **homoscedasticity** (or **homogeneity of variance**) is an important condition in parametric statistics.

```{r, eval=T}
# Test of variance: we test HO: homogeneous, H1:heterogeneous
fligner.test (students$height ~ students$gender)
```

Another example using the `ToothGrowth` data set (`car` package) on multiple groups

```{r, eval=T}
tg<-ToothGrowth
tg$dose<-factor(tg$dose)
boxplot(len~dose*supp, data=tg)
# also work with: boxplot(len ~ interaction (dose,supp), data=tg)
# or: plot(len ~ interaction (dose,supp), data=tg)
bartlett.test(len~interaction (supp,dose),data=ToothGrowth) # sensitivity non-normality +++
leveneTest(len~interaction (supp,dose),data=ToothGrowth) # sensitivity non-normality ++
fligner.test(len~interaction (supp,dose),data=ToothGrowth) # sensitivity non-normality +
```


<p class="alert">
**Practice 7.4** Create your __own__ `my.t.test` function (e.g. you will not use the above `t.test` function but you will code your own function). You will probably need to check at `?pt` in order to extract the p-value. You can look at `getAnywhere("t.test.default")` for the actual source code of the `t.test` function. Please use your customized function to test  the effect  of `treatment` on _length_ at various _days_ in the `rairuoho` dataset. Interpret.
</p>


